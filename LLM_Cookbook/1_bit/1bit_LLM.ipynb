{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/microsoft/BitNet.git\n",
        "%cd BitNet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nJxS8qPqMa4",
        "outputId": "b490eeda-9644-44a4-dc92-1e490bb75e26"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BitNet'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 124 (delta 52), reused 86 (delta 36), pack-reused 3 (from 1)\u001b[K\n",
            "Receiving objects: 100% (124/124), 1.88 MiB | 4.90 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n",
            "Submodule '3rdparty/llama.cpp' (https://github.com/Eddie-Wang1120/llama.cpp.git) registered for path '3rdparty/llama.cpp'\n",
            "Cloning into '/content/BitNet/BitNet/3rdparty/llama.cpp'...\n",
            "remote: Enumerating objects: 25578, done.        \n",
            "remote: Counting objects: 100% (5272/5272), done.        \n",
            "remote: Compressing objects: 100% (241/241), done.        \n",
            "remote: Total 25578 (delta 5173), reused 5031 (delta 5031), pack-reused 20306 (from 1)        \n",
            "Receiving objects: 100% (25578/25578), 55.95 MiB | 18.40 MiB/s, done.\n",
            "Resolving deltas: 100% (18457/18457), done.\n",
            "Submodule path '3rdparty/llama.cpp': checked out '814d0ee5440495255a4e3a5a8abf001b27b539d4'\n",
            "Submodule 'kompute' (https://github.com/nomic-ai/kompute.git) registered for path '3rdparty/llama.cpp/ggml/src/kompute'\n",
            "Cloning into '/content/BitNet/BitNet/3rdparty/llama.cpp/ggml/src/kompute'...\n",
            "remote: Enumerating objects: 9118, done.        \n",
            "remote: Counting objects: 100% (253/253), done.        \n",
            "remote: Compressing objects: 100% (148/148), done.        \n",
            "remote: Total 9118 (delta 119), reused 184 (delta 95), pack-reused 8865 (from 1)        \n",
            "Receiving objects: 100% (9118/9118), 17.59 MiB | 16.97 MiB/s, done.\n",
            "Resolving deltas: 100% (5726/5726), done.\n",
            "Submodule path '3rdparty/llama.cpp/ggml/src/kompute': checked out '4565194ed7c32d1d2efa32ceab4d3c6cae006306'\n",
            "/content/BitNet/BitNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "DM2VGH1kqREA",
        "outputId": "f790f8e1-39d4-4206-b58e-fbdaaac393b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n",
            "/bin/bash: line 1: conda: command not found\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: numpy~=1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: sentencepiece~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /usr/local/lib/python3.10/dist-packages (from -r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (4.46.2)\n",
            "Collecting gguf>=0.1.0 (from -r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 4))\n",
            "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /usr/local/lib/python3.10/dist-packages (from -r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.5)\n",
            "Collecting torch~=2.2.1 (from -r 3rdparty/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3))\n",
            "  Downloading https://download.pytorch.org/whl/cpu/torch-2.2.2%2Bcpu-cp310-cp310-linux_x86_64.whl (186.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.45.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.2.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.45.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.45.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.45.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.45.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.2.1->-r 3rdparty/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n",
            "Downloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gguf, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.2.2+cpu which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.2.2+cpu which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gguf-0.10.0 torch-2.2.2+cpu\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "functorch",
                  "torch",
                  "torchgen"
                ]
              },
              "id": "0e2a28cfb3d04bdc87880d8ffdf3e191"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model from Hugging Face, convert it to quantized gguf format, and build the project\n",
        "!python setup_env.py --hf-repo 1bitLLM/bitnet_b1_58-large -q i2_s\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlSEwkpDqiOp",
        "outputId": "74c4bd3d-c611-47ba-dc66-f778750c87bd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Compiling the code using CMake.\n",
            "INFO:root:Downloading model 1bitLLM/bitnet_b1_58-large from HuggingFace to models/bitnet_b1_58-large...\n",
            "INFO:root:Converting HF model to GGUF format...\n",
            "INFO:root:GGUF model saved at models/bitnet_b1_58-large/ggml-model-i2_s.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference with the quantized model\n",
        "!python run_inference.py -m models/bitnet_b1_58-large/ggml-model-i2_s.gguf -p \"Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?\\nAnswer:\" -n 6 -temp 0\n",
        "\n",
        "# Output:\n",
        "# Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?\n",
        "# Answer: Mary is in the garden.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDrfOBFpqqkG",
        "outputId": "1f812772-b7fc-4451-c573-0d76c3abc0a7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "warning: not compiled with GPU offload support, --gpu-layers option will be ignored\n",
            "warning: see main README.md for information on enabling GPU BLAS support\n",
            "build: 3948 (814d0ee5) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 26 key-value pairs and 266 tensors from models/bitnet_b1_58-large/ggml-model-i2_s.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = bitnet\n",
            "llama_model_loader: - kv   1:                               general.name str              = bitnet_b1_58-large\n",
            "llama_model_loader: - kv   2:                         bitnet.block_count u32              = 24\n",
            "llama_model_loader: - kv   3:                      bitnet.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                    bitnet.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv   5:                 bitnet.feed_forward_length u32              = 4096\n",
            "llama_model_loader: - kv   6:                bitnet.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv   7:             bitnet.attention.head_count_kv u32              = 16\n",
            "llama_model_loader: - kv   8:                      bitnet.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv   9:    bitnet.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 40\n",
            "llama_model_loader: - kv  11:                          bitnet.vocab_size u32              = 32002\n",
            "llama_model_loader: - kv  12:                   bitnet.rope.scaling.type str              = linear\n",
            "llama_model_loader: - kv  13:                 bitnet.rope.scaling.factor f32              = 1.000000\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   97 tensors\n",
            "llama_model_loader: - type  f16:    1 tensors\n",
            "llama_model_loader: - type i2_s:  168 tensors\n",
            "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
            "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 5\n",
            "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = bitnet\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32002\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 1536\n",
            "llm_load_print_meta: n_layer          = 24\n",
            "llm_load_print_meta: n_head           = 16\n",
            "llm_load_print_meta: n_head_kv        = 16\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1536\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1536\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 4096\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 700M\n",
            "llm_load_print_meta: model ftype      = I2_S - 2 bpw ternary\n",
            "llm_load_print_meta: model params     = 728.84 M\n",
            "llm_load_print_meta: model size       = 256.56 MiB (2.95 BPW) \n",
            "llm_load_print_meta: general.name     = bitnet_b1_58-large\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32000 '</line>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOG token        = 2 '</s>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.12 MiB\n",
            "llm_load_tensors:        CPU buffer size =   256.56 MiB\n",
            ".................................................................\n",
            "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
            "llama_new_context_with_model: n_ctx      = 2048\n",
            "llama_new_context_with_model: n_batch    = 32\n",
            "llama_new_context_with_model: n_ubatch   = 32\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     5.00 MiB\n",
            "llama_new_context_with_model: graph nodes  = 870\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 2\n",
            "\n",
            "system_info: n_threads = 2 (n_threads_batch = 2) / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "\n",
            "sampler seed: 4294967295\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.000\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> greedy \n",
            "generate: n_ctx = 2048, n_batch = 1, n_predict = 6, n_keep = 1\n",
            "\n",
            " Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?\n",
            "Answer: Mary went to the kitchen.\n",
            "\n",
            "llama_perf_sampler_print:    sampling time =       0.39 ms /    60 runs   (    0.01 ms per token, 153452.69 tokens per second)\n",
            "llama_perf_context_print:        load time =     367.62 ms\n",
            "llama_perf_context_print: prompt eval time =    2108.18 ms /    54 tokens (   39.04 ms per token,    25.61 tokens per second)\n",
            "llama_perf_context_print:        eval time =     187.67 ms /     5 runs   (   37.53 ms per token,    26.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    2301.41 ms /    59 tokens\n"
          ]
        }
      ]
    }
  ]
}